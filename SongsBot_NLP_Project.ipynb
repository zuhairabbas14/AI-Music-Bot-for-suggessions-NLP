{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_project(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o57RiR4uRYQP"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**IMPORTS**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQU4lthwRc-K",
        "outputId": "f2d8ced6-5e07-474b-a820-acd6a7b5fe32"
      },
      "source": [
        "!pip install wolframalpha api\r\n",
        "\r\n",
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from nltk.tokenize import TweetTokenizer \r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "import wolframalpha\r\n",
        "import random\r\n",
        "from collections import defaultdict\r\n",
        "from nltk import ClassifierBasedTagger\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "from nltk import ChunkParserI, tree2conlltags, conlltags2tree\r\n",
        "from nltk.metrics import accuracy\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wolframalpha\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/1f/b80000a9307e7925ecbafb169d3b4d687b155d5f377b1e8f33f1d4351a9c/wolframalpha-4.1.1-py3-none-any.whl\n",
            "Collecting api\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/e0/6b3451133002547c198c89af9a71577b186086a010afe709613c99d1b3dd/api-0.0.7.tar.gz\n",
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from wolframalpha) (8.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from api) (2.23.0)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->api) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->api) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->api) (1.24.3)\n",
            "Building wheels for collected packages: api\n",
            "  Building wheel for api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for api: filename=api-0.0.7-cp36-none-any.whl size=2327 sha256=453f0d88619c43bee50bb87937afbd99e720096ea2d804b77432e54e615733b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/b3/13/6fb6612d6dd2d4bc2dfa783e9be7554e7c2f276912d3cb850e\n",
            "Successfully built api\n",
            "Installing collected packages: xmltodict, wolframalpha, nose, api\n",
            "Successfully installed api-0.0.7 nose-1.3.7 wolframalpha-4.1.1 xmltodict-0.12.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N8wBptGBthy"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**CUSTOM TRAINING DATASET**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X8FICa8Byzx"
      },
      "source": [
        "TRAINING_SET = {\r\n",
        "    \r\n",
        "# General Intents\r\n",
        "\r\n",
        "'greetings': [\r\n",
        "  (\"Hello!\", {}),\r\n",
        "  (\"Hi!\", {}),\r\n",
        "  (\"How are you?\", {}),\r\n",
        "  (\"Hi There!\", {}),\r\n",
        "  (\"Hello there!!!\", {}),\r\n",
        "  (\"Ello!\", {}),\r\n",
        "  (\"Hey!\", {}),\r\n",
        "  (\"Hello mate, how are you?\", {}),\r\n",
        "  (\"Good morning!\", {}),\r\n",
        "  (\"mornin'\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'thanks': [ \r\n",
        "  (\"Thanks dude!\", {}),\r\n",
        "  (\"Thank you!\", {}),\r\n",
        "  (\"Thanks\", {}),\r\n",
        "  (\"awesome, thanks for your help\", {}),\r\n",
        "  (\"Thank you so much!\", {}),\r\n",
        "  (\"Cheers!\", {}),\r\n",
        "  (\"Cheers, thanks\", {}),\r\n",
        "  (\"many thanks\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'bye': [\r\n",
        "  (\"Bye!\", {}),\r\n",
        "  (\"Bye Bye!\", {}),\r\n",
        "  (\"ByeBye!\", {}),\r\n",
        "  (\"Bbye!\", {}), (\"See you!\", {}), (\"See you later\", {}), (\"Good bye\", {}), (\"See you soon\", {}), (\"Talk to you later!\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'rude': [ \r\n",
        "  (\"You're an idiot\", {}),\r\n",
        "  (\"You're such a fucking idiot!\", {}),\r\n",
        "  (\"You're so stupid !!!\", {}),\r\n",
        "  (\"You stupid fuck !!!\", {}),\r\n",
        "  (\"Shut up!\", {}),\r\n",
        "  (\"Shut the fuck up you fuckhead!\", {}),\r\n",
        "  (\"Go to hell!\", {}),\r\n",
        "  (\"shit!!\", {}),\r\n",
        "  (\"You dumbass!!\", {}),\r\n",
        "  (\"fuck you!!\", {}),\r\n",
        "  (\"haha, you stupid !\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'chit-chat': [ \r\n",
        "  (\"How are you?\", {}),\r\n",
        "  (\"Hey, how are you?\", {}),\r\n",
        "  (\"How's life?\", {}),\r\n",
        "  (\"How you've been?\", {}),\r\n",
        "  (\"How's your day?\", {}),\r\n",
        "  (\"How's life?\", {}),\r\n",
        "  (\"How's business?\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'ask-info': [ \r\n",
        "  (\"Tell me more about yourself\", {}),\r\n",
        "  (\"Who are you?\", {}),\r\n",
        "  (\"Can you provide more info?\", {}),\r\n",
        "  (\"Can I get some information?\", {}),\r\n",
        "  (\"Give me more info\", {}),\r\n",
        "  (\"Give me some info please\", {}),\r\n",
        "  (\"I need some info\", {}),\r\n",
        "  (\"I would like to know more please\", {}),\r\n",
        "  (\"Tell me about you\", {}),\r\n",
        "  (\"Let's talk about you\", {}),\r\n",
        "  (\"Who are you?\", {}),\r\n",
        "],\r\n",
        " \r\n",
        "# Song based intents \r\n",
        "\r\n",
        "'what-singer': [ \r\n",
        "  (\"Who sang Sugar?\", {'song': 'Sugar'}),\r\n",
        "  (\"Who sang Bohemian Rhapsody?\", {'song': 'bohemian rhapsody'}),\r\n",
        "  (\"Who sang Halo?\", {'song': 'Halo'}), \r\n",
        "  (\"Who is the singer of We Are One?\", {'song': 'We Are One'}), \r\n",
        "  (\"The singer of Cold is?\", {'song': 'Cold'}), \r\n",
        "  (\"singer of Moves Like Jagger is?\", {'song': 'Moves Like Jagger'}), \r\n",
        "  (\"Who sang Killer Queen?\", {'song': 'Killer Queen'}), \r\n",
        "  (\"Who is the singer of Senorita?\", {'song': 'Senorita'}),\r\n",
        "],\r\n",
        "\r\n",
        "'what-release-year': [ \r\n",
        "  (\"In what year was We are the Champions released?\", {'song': 'We are the Champions'}),\r\n",
        "  (\"In what year was Hello released?\", {'song': 'Hello'}),\r\n",
        "  (\"When was Somebody to Love released?\", {'song': 'Somebody to Love'}),\r\n",
        "  (\"When was Time is Running Out released?\", {'song': 'Time is Running Out'}),\r\n",
        "  (\"When was Gangam Style produced?\", {'song': 'Gangam Style'}),\r\n",
        "  (\"What's the release year for Attention?\", {'song': 'Attention'}),\r\n",
        "  (\"What's the release year for Numa Numa?\",  {'song': 'Numa Numa'}),\r\n",
        "],\r\n",
        "\r\n",
        "'get-similar-songs': [ \r\n",
        "  (\"What are some similar songs to Memories\", {'song': 'Memories'}),\r\n",
        "  (\"What are some similar song to Welcome to the Jungle?\", {'song': 'Welcome to the Jungle'}),\r\n",
        "  (\"Can I get some similar song to Baby?\", {'song': 'Baby'}),\r\n",
        "  (\"I want more song like Home Sweet Home ?\", {'song': 'Home Sweet Home'}),\r\n",
        "  (\"I want more song like Not Afraid ?\", {'song': 'Not Afraid'}),\r\n",
        "  (\"I want more song similar to Last Christmas ?\", {'song': 'Last Christmas'}),\r\n",
        "  (\"I want to know more song similar to Complicated ?\", {'song': 'Complicated'}),\r\n",
        "  (\"Get me more song like Numb ?\", {'song': 'Numb'}),\r\n",
        "  (\"What are some song similar to In the End ?\", {'song': 'In the End'}),\r\n",
        "],\r\n",
        "\r\n",
        "'get-latest': [ \r\n",
        "  (\"What are some new songs?\", {}),\r\n",
        "  (\"What are some new released songs?\", {}),\r\n",
        "  (\"What are some new releases?\", {}),\r\n",
        "  (\"Can I get some new songs?\", {}),\r\n",
        "  (\"What's new?\", {}),\r\n",
        "  (\"What are some new and cool songs?\", {}),\r\n",
        "  (\"Tell me about some new songs?\", {}),\r\n",
        "  (\"Tell me about new releases?\", {}),\r\n",
        "],\r\n",
        "\r\n",
        "'singer-songs': [ \r\n",
        "  ('What songs did Ryan Gosling sing?', {'singer': 'Ryan Gosling'}),\r\n",
        "  ('in which songs did Selena Gomez play?', {'singer': 'Selena Gomez'}),\r\n",
        "  ('In which songs did Jason Momoa play?', {'singer': 'Jason Momoa'}),\r\n",
        "  ('What are some songs in which Emilia Clarke acted ?', {'singer': 'Emilia Clarke'}),\r\n",
        "  ('What are some songs which was sung by Johnny Depp ?', {'singer': 'Johnny Depp'}),\r\n",
        "  ('Tell me some songs by Johnny Depp ?', {'singer': 'Johnny Depp'}),\r\n",
        "  ('Tell me some songs from Tom Cruise ?', {'singer': 'Tom Cruise'}),\r\n",
        "  ('Get me some songs sung by Leonardo DiCaprio ?', {'singer': 'Leonardo DiCaprio'}),\r\n",
        "  ('Some songs by Matt Damon ?', {'singer': 'Matt Damon'}),\r\n",
        "  ('Some cool songs by Keanu Reeves ?', {'singer': 'Keanu Reeves'}),\r\n",
        "],\r\n",
        "\r\n",
        "'song-team': [\r\n",
        "  (\"What's the team of Way Down We Go ?\", {'song': 'Way Down We Go'}),\r\n",
        "  (\"What's the cast of Santa Tell Me\", {'song': 'Santa Tell Me'}),\r\n",
        "  (\"Who worked in Sugar?\", {'song': 'Sugar'}),\r\n",
        "  (\"Who composed City of Stars ?\", {'song': 'City of Stars'}),\r\n",
        "  (\"Who produced Naked?\", {'song': 'Naked'}),\r\n",
        "  (\"Who worked on Shallow?\", {'song': 'Shallow'}),\r\n",
        "],\r\n",
        "\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSDK0LfgRiR4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**UTILITY CLASSES & FUNCTIONS**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-kS2noabosE"
      },
      "source": [
        "tokenizer = TweetTokenizer()\r\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfoMbs6ZVyey"
      },
      "source": [
        "class ClassifierBasedChunkParser(ChunkParserI):\r\n",
        "  def __init__(self, chunked_sents, feature_detector, classifier_builder, **kwargs):\r\n",
        "    # Transform the trees in IOB annotated sentences [(word, pos, chunk), ...]\r\n",
        "    chunked_sents = [tree2conlltags(sent) for sent in chunked_sents]\r\n",
        "    chunked_sents = [triplets2tagged_pairs(sent) for sent in chunked_sents]\r\n",
        "    self.feature_detector = feature_detector\r\n",
        "    self.tagger = ClassifierBasedTaggerBatchTrained(\r\n",
        "    train=(sent for sent in chunked_sents),\r\n",
        "    feature_detector=self.feature_detector,\r\n",
        "    classifier_builder=classifier_builder\r\n",
        "    )\r\n",
        "\r\n",
        "  def parse(self, tagged_sent):\r\n",
        "    chunks = self.tagger.tag(tagged_sent)\r\n",
        "    iob_triplets = tagged_pairs2triplets(chunks)\r\n",
        "    # Transform the list of triplets to nltk.Tree format\r\n",
        "    return conlltags2tree(iob_triplets)\r\n",
        "\r\n",
        "  def evaluate(self, gold):\r\n",
        "    # Convert nltk.Tree chunked sentences to (word, pos, iob) triplets\r\n",
        "    chunked_sents = [tree2conlltags(sent) for sent in gold]\r\n",
        "    # Convert (word, pos, iob) triplets to tagged tuples ((word, pos), iob)\r\n",
        "    chunked_sents = [triplets2tagged_pairs(sent) for sent in chunked_sents]\r\n",
        "    print(chunked_sents)\r\n",
        "    dataset = self.tagger._todataset(chunked_sents)\r\n",
        "    featuresets, tags = zip(*dataset)\r\n",
        "    predicted_tags = self.tagger.classifier().classify_many(featuresets)\r\n",
        "    return accuracy(tags, predicted_tags)\r\n",
        "\r\n",
        "\r\n",
        "class ScikitClassifier(nltk.ClassifierI):\r\n",
        "  \"\"\"\r\n",
        "  Wrapper over a scikit-learn classifier\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, classifier=None, vectorizer=None, model=None):\r\n",
        "    if model is None:\r\n",
        "      if vectorizer is None:\r\n",
        "        vectorizer = DictVectorizer(sparse=False)\r\n",
        "      if classifier is None:\r\n",
        "        classifier = LogisticRegression()\r\n",
        "      self.model = Pipeline([\r\n",
        "      ('vectorizer', vectorizer),\r\n",
        "      ('classifier', classifier)\r\n",
        "      ])\r\n",
        "    else:\r\n",
        "      self.model = model\r\n",
        "\r\n",
        "  @property\r\n",
        "  def vectorizer(self):\r\n",
        "    return self.model[0][1]\r\n",
        "\r\n",
        "  @property\r\n",
        "  def classifier(self):\r\n",
        "    return self.model[1][1]\r\n",
        "\r\n",
        "  def train(self, featuresets, labels):\r\n",
        "    self.model.fit(featuresets, labels)\r\n",
        "\r\n",
        "  def partial_train(self, featuresets, labels, all_labels):\r\n",
        "    self.model.partial_fit(featuresets, labels, all_labels)\r\n",
        "\r\n",
        "  def test(self, featuresets, labels):\r\n",
        "    self.model.score(featuresets, labels)\r\n",
        "\r\n",
        "  def labels(self):\r\n",
        "    return list(self.model.steps[1][1].classes_)\r\n",
        "\r\n",
        "  def classify(self, featureset):\r\n",
        "    return self.model.predict([featureset])[0]\r\n",
        "\r\n",
        "  def classify_many(self, featuresets):\r\n",
        "    return self.model.predict(featuresets)\r\n",
        "\r\n",
        "\r\n",
        "class ClassifierBasedTaggerBatchTrained(ClassifierBasedTagger):\r\n",
        "  def _todataset(self, tagged_sentences):\r\n",
        "    classifier_corpus = []\r\n",
        "    for sentence in tagged_sentences:\r\n",
        "      history = []\r\n",
        "      untagged_sentence, tags = zip(*sentence)\r\n",
        "      for index in range(len(sentence)):\r\n",
        "        featureset = self.feature_detector(untagged_sentence,\r\n",
        "        index, history)\r\n",
        "        classifier_corpus.append((featureset, tags[index]))\r\n",
        "        history.append(tags[index])\r\n",
        "    return classifier_corpus\r\n",
        "\r\n",
        "  def _train(self, tagged_corpus, classifier_builder, verbose):\r\n",
        "    \"\"\"\r\n",
        "    Build a new classifier, based on the given training data\r\n",
        "    *tagged_corpus*.\r\n",
        "    \"\"\"\r\n",
        "    if verbose:\r\n",
        "      print('Constructing training corpus for classifier.')\r\n",
        "    self._classifier = classifier_builder(tagged_corpus, lambda sents: self._todataset(sents))\r\n",
        "  \r\n",
        "  def evaluate(self, gold):\r\n",
        "    dataset = self._todataset(gold)\r\n",
        "    featuresets, tags = zip(*dataset)\r\n",
        "    predicted_tags = self.classifier().classify_many(featuresets)\r\n",
        "    return accuracy(tags, predicted_tags)\r\n",
        "\r\n",
        "\r\n",
        "def chunk_features(tokens, index, history):\r\n",
        "  \"\"\"\r\n",
        "  `tokens` = a POS-tagged sentence [(w1, t1), ...]\r\n",
        "  `index` = the index of the token we want to extract features for\r\n",
        "  `history` = the previous predicted IOB tags\r\n",
        "  \"\"\"\r\n",
        "  # Pad the sequence with placeholders\r\n",
        "  tokens = ([('__START2__', '__START2__'), ('__START1__', '__START1__')] +\r\n",
        "  list(tokens) +\r\n",
        "  [('__END1__', '__END1__'), ('__END2__', '__END2__')])\r\n",
        "  history = ['__START2__', '__START1__'] + list(history)\r\n",
        "  # shift the index with 2, to accommodate the padding\r\n",
        "  index += 2\r\n",
        "  word, pos = tokens[index]\r\n",
        "  prevword, prevpos = tokens[index - 1]\r\n",
        "  prevprevword, prevprevpos = tokens[index - 2]\r\n",
        "  nextword, nextpos = tokens[index + 1]\r\n",
        "  nextnextword, nextnextpos = tokens[index + 2]\r\n",
        "  return {\r\n",
        "  'word': word,\r\n",
        "  'lemma': stemmer.stem(word),\r\n",
        "  'pos': pos,\r\n",
        "  'next-word': nextword,\r\n",
        "  'next-pos': nextpos,\r\n",
        "  'next-next-word': nextnextword,\r\n",
        "  'nextnextpos': nextnextpos,\r\n",
        "  'prev-word': prevword,\r\n",
        "  'prev-pos': prevpos,\r\n",
        "  'prev-prev-word': prevprevword,\r\n",
        "  'prev-prev-pos': prevprevpos,\r\n",
        "  # Historical features\r\n",
        "  'prev-chunk': history[-1],\r\n",
        "  'prev-prev-chunk': history[-2],\r\n",
        "  }\r\n",
        "\r\n",
        "def triplets2tagged_pairs(iob_sent):\r\n",
        "  \"\"\"\r\n",
        "  Transform the triplets to tagged pairs:\r\n",
        "  [(word1, pos1, iob1), (word2, pos2, iob2), ...] ->\r\n",
        "  [((word1, pos1), iob1), ((word2, pos2), iob2),...]\r\n",
        "  \"\"\"\r\n",
        "  return [((word, pos), chunk) for word, pos, chunk in iob_sent]\r\n",
        "\r\n",
        "def tagged_pairs2triplets(iob_sent):\r\n",
        "  \"\"\"\r\n",
        "  Transform the triplets to tagged pairs:\r\n",
        "  [((word1, pos1), iob1), ((word2, pos2), iob2),...] ->\r\n",
        "  [(word1, pos1, iob1), (word2, pos2, iob2), ...]\r\n",
        "  \"\"\"\r\n",
        "  return [(word, pos, chunk) for (word, pos), chunk in iob_sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvfr-SwWRhiD"
      },
      "source": [
        "def train_intent_model(train_set):\r\n",
        "  pipeline = Pipeline(steps=[\r\n",
        "  ('vectorizer', CountVectorizer(tokenizer=tokenizer.tokenize)),\r\n",
        "  ('classifier', MultinomialNB()),\r\n",
        "  ])\r\n",
        "  dataset = []\r\n",
        "  for intent, samples in train_set.items():\r\n",
        "    for sample in samples:\r\n",
        "      dataset.append((sample[0], intent))\r\n",
        "  random.shuffle(dataset)\r\n",
        "  intent_X, intent_y = zip(*dataset)\r\n",
        "  pipeline.fit(intent_X, intent_y)\r\n",
        "  print(\"Accuracy on training set:\", pipeline.score(intent_X, intent_y))\r\n",
        "  return pipeline\r\n",
        "\r\n",
        "def sub_list(lst, slst):\r\n",
        "  \"\"\"\r\n",
        "  Utility function for getting\r\n",
        "  the index of a sublist inside a list\r\n",
        "  \"\"\"\r\n",
        "  start_index = 0\r\n",
        "  while len(lst) > len(slst):\r\n",
        "    if lst[:len(slst)] == slst:\r\n",
        "      return start_index\r\n",
        "    lst, start_index = lst[1:], start_index + 1\r\n",
        "  return None\r\n",
        "\r\n",
        "def mark_entities(tagged_sentence, entity_words, label):\r\n",
        "  \"\"\"\r\n",
        "  tagged_sentence: [('Word', 'Tag'), ...]\r\n",
        "  entity_words: ['This', 'is', 'an', 'entity']\r\n",
        "  label: the entity type\r\n",
        "  return a nltk.Tree instance with the entities wrapped in chunks\r\n",
        "  \"\"\"\r\n",
        "  iob_tagged = [(w, t, 'O') for w, t in tagged_sentence]\r\n",
        "  words = nltk.untag(tagged_sentence)\r\n",
        "  start_index = sub_list(words, entity_words)\r\n",
        "  if start_index is not None:\r\n",
        "    iob_tagged[start_index] = (\r\n",
        "    iob_tagged[start_index][0],\r\n",
        "    iob_tagged[start_index][1],\r\n",
        "    'B-' + label\r\n",
        "    )\r\n",
        "    for idx in range(1, len(entity_words)):\r\n",
        "      iob_tagged[start_index + idx] = (\r\n",
        "      iob_tagged[start_index + idx][0],\r\n",
        "      iob_tagged[start_index + idx][1],\r\n",
        "      'I-' + label\r\n",
        "      )\r\n",
        "  return nltk.conlltags2tree(iob_tagged)\r\n",
        "\r\n",
        "def build_extractors_datasets(train_set):\r\n",
        "  \"\"\"\r\n",
        "  Transform the training set from the original form nltk.Tree organized by entity_type\r\n",
        "  {entity_type: [tree1, tree2, ...]}\r\n",
        "  \"\"\"\r\n",
        "  tokenizer = TweetTokenizer()\r\n",
        "  datasets = defaultdict(list)\r\n",
        "  for _, samples in train_set.items():\r\n",
        "    for sample in samples:\r\n",
        "      words = tokenizer.tokenize(sample[0])\r\n",
        "      tagged = nltk.pos_tag(words)\r\n",
        "      for entity_type, entity in sample[1].items():\r\n",
        "        entity_words = tokenizer.tokenize(entity)\r\n",
        "        tree = mark_entities(tagged, entity_words, entity_type)\r\n",
        "        datasets[entity_type].append(tree)\r\n",
        "  return datasets\r\n",
        "\r\n",
        "def build_intent_extractor_mapping(train_set):\r\n",
        "  mapping = {}\r\n",
        "  for intent in train_set:\r\n",
        "    mapping[intent] = set({})\r\n",
        "    for sample in train_set[intent]:\r\n",
        "      mapping[intent].update(list(sample[1].keys()))\r\n",
        "  return mapping\r\n",
        "\r\n",
        "def train_extractor(trees):\r\n",
        "  def train_scikit_classifier(sentences, feature_detector):\r\n",
        "    dataset = feature_detector(sentences)\r\n",
        "    featuresets, labels = zip(*dataset)\r\n",
        "    scikit_classifier = ScikitClassifier()\r\n",
        "    scikit_classifier.train(featuresets, labels)\r\n",
        "    return scikit_classifier\r\n",
        "  classifier_chunker = ClassifierBasedChunkParser(\r\n",
        "  trees,\r\n",
        "  chunk_features,\r\n",
        "  train_scikit_classifier,\r\n",
        "  )\r\n",
        "  print(\"Accuracy on training set:\", classifier_chunker.evaluate(trees))\r\n",
        "  return classifier_chunker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6KWvvF5Y78"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**CHAT BOT CLASS**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7j8UhTwUTlN"
      },
      "source": [
        "class Chatbot(object):\r\n",
        "\r\n",
        "  def __init__(self, min_intent_confidence=.2):\r\n",
        "    self.min_intent_confidence = min_intent_confidence\r\n",
        "    self.intent_model = None\r\n",
        "    self.entity_types = []\r\n",
        "    self.extractors = {}\r\n",
        "    self.intent_extractor_mapping = None\r\n",
        "    self.handlers = {}\r\n",
        "\r\n",
        "  def train(self, training_set):\r\n",
        "    # Train intent model\r\n",
        "    self.intent_model = train_intent_model(training_set)\r\n",
        "    extractor_datasets = build_extractors_datasets(training_set)\r\n",
        "    self.intent_extractor_mapping = build_intent_extractor_mapping(training_set)\r\n",
        "    # Train extractor for each entity type\r\n",
        "    for entity_type, data in extractor_datasets.items():\r\n",
        "      self.extractors[entity_type] = train_extractor(data)\r\n",
        "\r\n",
        "  def predict_intent(self, line):\r\n",
        "    probs = self.intent_model.predict_proba([line])[0]\r\n",
        "    best_score_index = probs.argmax()\r\n",
        "    best_score = probs[best_score_index]\r\n",
        "    if best_score < self.min_intent_confidence:\r\n",
        "      return None\r\n",
        "    return self.intent_model.classes_[best_score_index]\r\n",
        "\r\n",
        "  def predict_entities(self, line, intent):\r\n",
        "    tagged = nltk.pos_tag(tokenizer.tokenize(line))\r\n",
        "    # Get applicable extractors for the intent\r\n",
        "    applicable_extractors = self.intent_extractor_mapping[intent]\r\n",
        "    # Apply the extractors and get the entities\r\n",
        "    entities = {}\r\n",
        "    for extractor_name in applicable_extractors:\r\n",
        "      tree = self.extractors[extractor_name].parse(tagged)\r\n",
        "      extracted_entities = [t for t in tree if isinstance(t, nltk.Tree)]\r\n",
        "      first_entity = None\r\n",
        "      if extracted_entities:\r\n",
        "        first_entity = extracted_entities[0]\r\n",
        "      if first_entity != None:\r\n",
        "        entities[extractor_name] = ' '.join(t[0] for t in first_entity)\r\n",
        "    return entities\r\n",
        "\r\n",
        "  def handle(self, intent, entities):\r\n",
        "    assert intent in self.handlers, \"Register a handler for intent: %s\" % intent\r\n",
        "    return self.handlers[intent](entities)\r\n",
        "\r\n",
        "  def register_handler(self, intent):\r\n",
        "    \"\"\" Decorator for registering an intent handler \"\"\"\r\n",
        "    def wrapper(handler):\r\n",
        "      self.handlers[intent] = handler\r\n",
        "    return wrapper\r\n",
        "\r\n",
        "  def register_default_handler(self):\r\n",
        "    \"\"\" Decorator for registering the default (fallback) handler \"\"\"\r\n",
        "    def wrapper(handler):\r\n",
        "      self.handlers[None] = handler\r\n",
        "    return wrapper\r\n",
        "\r\n",
        "  def tell(self, line):\r\n",
        "    intent = self.predict_intent(line)\r\n",
        "    print(\"> Intent: {}\".format(intent))\r\n",
        "    # In case we haven't registered a handler for this intent\r\n",
        "    # let's just pretend we just don't know better\r\n",
        "    if intent not in self.handlers:\r\n",
        "      intent = None\r\n",
        "    if intent is not None:\r\n",
        "      entities = self.predict_entities(line, intent)\r\n",
        "    else:\r\n",
        "      entities = {}\r\n",
        "    return self.handle(intent, entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgiSoVBQ3Sqw",
        "outputId": "fc0fb104-d453-4052-ae14-d707c2e1d016"
      },
      "source": [
        "bot = Chatbot()\n",
        "bot.train(TRAINING_SET)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 0.9519230769230769\n",
            "[[(('Who', 'WP'), 'O'), (('sang', 'VBD'), 'O'), (('Sugar', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('sang', 'VBD'), 'O'), (('Bohemian', 'NNP'), 'O'), (('Rhapsody', 'NNP'), 'O'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('sang', 'VBD'), 'O'), (('Halo', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('is', 'VBZ'), 'O'), (('the', 'DT'), 'O'), (('singer', 'NN'), 'O'), (('of', 'IN'), 'O'), (('We', 'PRP'), 'B-song'), (('Are', 'VBP'), 'I-song'), (('One', 'CD'), 'I-song'), (('?', '.'), 'O')], [(('The', 'DT'), 'O'), (('singer', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Cold', 'NNP'), 'B-song'), (('is', 'VBZ'), 'O'), (('?', '.'), 'O')], [(('singer', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Moves', 'NNS'), 'B-song'), (('Like', 'IN'), 'I-song'), (('Jagger', 'NNP'), 'I-song'), (('is', 'VBZ'), 'O'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('sang', 'VBD'), 'O'), (('Killer', 'NNP'), 'B-song'), (('Queen', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('is', 'VBZ'), 'O'), (('the', 'DT'), 'O'), (('singer', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Senorita', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('In', 'IN'), 'O'), (('what', 'WP'), 'O'), (('year', 'NN'), 'O'), (('was', 'VBD'), 'O'), (('We', 'PRP'), 'B-song'), (('are', 'VBP'), 'I-song'), (('the', 'DT'), 'I-song'), (('Champions', 'NNS'), 'I-song'), (('released', 'VBN'), 'O'), (('?', '.'), 'O')], [(('In', 'IN'), 'O'), (('what', 'WP'), 'O'), (('year', 'NN'), 'O'), (('was', 'VBD'), 'O'), (('Hello', 'NNP'), 'B-song'), (('released', 'VBN'), 'O'), (('?', '.'), 'O')], [(('When', 'WRB'), 'O'), (('was', 'VBD'), 'O'), (('Somebody', 'NN'), 'B-song'), (('to', 'TO'), 'I-song'), (('Love', 'VB'), 'I-song'), (('released', 'VBN'), 'O'), (('?', '.'), 'O')], [(('When', 'WRB'), 'O'), (('was', 'VBD'), 'O'), (('Time', 'NNP'), 'B-song'), (('is', 'VBZ'), 'I-song'), (('Running', 'VBG'), 'I-song'), (('Out', 'RP'), 'I-song'), (('released', 'VBN'), 'O'), (('?', '.'), 'O')], [(('When', 'WRB'), 'O'), (('was', 'VBD'), 'O'), (('Gangam', 'NNP'), 'B-song'), (('Style', 'NNP'), 'I-song'), (('produced', 'VBD'), 'O'), (('?', '.'), 'O')], [((\"What's\", 'IN'), 'O'), (('the', 'DT'), 'O'), (('release', 'NN'), 'O'), (('year', 'NN'), 'O'), (('for', 'IN'), 'O'), (('Attention', 'NNP'), 'B-song'), (('?', '.'), 'O')], [((\"What's\", 'IN'), 'O'), (('the', 'DT'), 'O'), (('release', 'NN'), 'O'), (('year', 'NN'), 'O'), (('for', 'IN'), 'O'), (('Numa', 'NNP'), 'B-song'), (('Numa', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('What', 'WP'), 'O'), (('are', 'VBP'), 'O'), (('some', 'DT'), 'O'), (('similar', 'JJ'), 'O'), (('songs', 'NNS'), 'O'), (('to', 'TO'), 'O'), (('Memories', 'NNS'), 'O')], [(('What', 'WP'), 'O'), (('are', 'VBP'), 'O'), (('some', 'DT'), 'O'), (('similar', 'JJ'), 'O'), (('song', 'NN'), 'O'), (('to', 'TO'), 'O'), (('Welcome', 'VB'), 'B-song'), (('to', 'TO'), 'I-song'), (('the', 'DT'), 'I-song'), (('Jungle', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('Can', 'MD'), 'O'), (('I', 'PRP'), 'O'), (('get', 'VB'), 'O'), (('some', 'DT'), 'O'), (('similar', 'JJ'), 'O'), (('song', 'NN'), 'O'), (('to', 'TO'), 'O'), (('Baby', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('I', 'PRP'), 'O'), (('want', 'VBP'), 'O'), (('more', 'RBR'), 'O'), (('song', 'JJ'), 'O'), (('like', 'IN'), 'O'), (('Home', 'NNP'), 'B-song'), (('Sweet', 'NNP'), 'I-song'), (('Home', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('I', 'PRP'), 'O'), (('want', 'VBP'), 'O'), (('more', 'RBR'), 'O'), (('song', 'NNS'), 'O'), (('like', 'IN'), 'O'), (('Not', 'RB'), 'B-song'), (('Afraid', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('I', 'PRP'), 'O'), (('want', 'VBP'), 'O'), (('more', 'JJR'), 'O'), (('song', 'JJ'), 'O'), (('similar', 'JJ'), 'O'), (('to', 'TO'), 'O'), (('Last', 'JJ'), 'B-song'), (('Christmas', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('I', 'PRP'), 'O'), (('want', 'VBP'), 'O'), (('to', 'TO'), 'O'), (('know', 'VB'), 'O'), (('more', 'JJR'), 'O'), (('song', 'JJ'), 'O'), (('similar', 'JJ'), 'O'), (('to', 'TO'), 'O'), (('Complicated', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('Get', 'VB'), 'O'), (('me', 'PRP'), 'O'), (('more', 'RBR'), 'O'), (('song', 'JJ'), 'O'), (('like', 'IN'), 'O'), (('Numb', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('What', 'WP'), 'O'), (('are', 'VBP'), 'O'), (('some', 'DT'), 'O'), (('song', 'NN'), 'O'), (('similar', 'JJ'), 'O'), (('to', 'TO'), 'O'), (('In', 'IN'), 'B-song'), (('the', 'DT'), 'I-song'), (('End', 'NN'), 'I-song'), (('?', '.'), 'O')], [((\"What's\", 'IN'), 'O'), (('the', 'DT'), 'O'), (('team', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Way', 'NNP'), 'B-song'), (('Down', 'NNP'), 'I-song'), (('We', 'PRP'), 'I-song'), (('Go', 'NNP'), 'I-song'), (('?', '.'), 'O')], [((\"What's\", 'IN'), 'O'), (('the', 'DT'), 'O'), (('cast', 'NN'), 'O'), (('of', 'IN'), 'O'), (('Santa', 'NNP'), 'O'), (('Tell', 'NNP'), 'O'), (('Me', 'NNP'), 'O')], [(('Who', 'WP'), 'O'), (('worked', 'VBD'), 'O'), (('in', 'IN'), 'O'), (('Sugar', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('composed', 'VBD'), 'O'), (('City', 'NNP'), 'B-song'), (('of', 'IN'), 'I-song'), (('Stars', 'NNP'), 'I-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('produced', 'VBD'), 'O'), (('Naked', 'NNP'), 'B-song'), (('?', '.'), 'O')], [(('Who', 'WP'), 'O'), (('worked', 'VBD'), 'O'), (('on', 'IN'), 'O'), (('Shallow', 'NNP'), 'B-song'), (('?', '.'), 'O')]]\n",
            "Accuracy on training set: 1.0\n",
            "[[(('What', 'WP'), 'O'), (('songs', 'VBZ'), 'O'), (('did', 'VBD'), 'O'), (('Ryan', 'NNP'), 'B-singer'), (('Gosling', 'VBG'), 'I-singer'), (('sing', 'NN'), 'O'), (('?', '.'), 'O')], [(('in', 'IN'), 'O'), (('which', 'WDT'), 'O'), (('songs', 'NNS'), 'O'), (('did', 'VBD'), 'O'), (('Selena', 'NNP'), 'B-singer'), (('Gomez', 'NNP'), 'I-singer'), (('play', 'NN'), 'O'), (('?', '.'), 'O')], [(('In', 'IN'), 'O'), (('which', 'WDT'), 'O'), (('songs', 'NNS'), 'O'), (('did', 'VBD'), 'O'), (('Jason', 'NNP'), 'B-singer'), (('Momoa', 'NNP'), 'I-singer'), (('play', 'NN'), 'O'), (('?', '.'), 'O')], [(('What', 'WP'), 'O'), (('are', 'VBP'), 'O'), (('some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('in', 'IN'), 'O'), (('which', 'WDT'), 'O'), (('Emilia', 'NNP'), 'B-singer'), (('Clarke', 'NNP'), 'I-singer'), (('acted', 'VBD'), 'O'), (('?', '.'), 'O')], [(('What', 'WP'), 'O'), (('are', 'VBP'), 'O'), (('some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('which', 'WDT'), 'O'), (('was', 'VBD'), 'O'), (('sung', 'VBN'), 'O'), (('by', 'IN'), 'O'), (('Johnny', 'NNP'), 'B-singer'), (('Depp', 'NNP'), 'I-singer'), (('?', '.'), 'O')], [(('Tell', 'VB'), 'O'), (('me', 'PRP'), 'O'), (('some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('by', 'IN'), 'O'), (('Johnny', 'NNP'), 'B-singer'), (('Depp', 'NNP'), 'I-singer'), (('?', '.'), 'O')], [(('Tell', 'VB'), 'O'), (('me', 'PRP'), 'O'), (('some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('from', 'IN'), 'O'), (('Tom', 'NNP'), 'B-singer'), (('Cruise', 'NNP'), 'I-singer'), (('?', '.'), 'O')], [(('Get', 'VB'), 'O'), (('me', 'PRP'), 'O'), (('some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('sung', 'VBN'), 'O'), (('by', 'IN'), 'O'), (('Leonardo', 'NNP'), 'B-singer'), (('DiCaprio', 'NNP'), 'I-singer'), (('?', '.'), 'O')], [(('Some', 'DT'), 'O'), (('songs', 'NNS'), 'O'), (('by', 'IN'), 'O'), (('Matt', 'NNP'), 'B-singer'), (('Damon', 'NNP'), 'I-singer'), (('?', '.'), 'O')], [(('Some', 'DT'), 'O'), (('cool', 'JJ'), 'O'), (('songs', 'NNS'), 'O'), (('by', 'IN'), 'O'), (('Keanu', 'NNP'), 'B-singer'), (('Reeves', 'NNP'), 'I-singer'), (('?', '.'), 'O')]]\n",
            "Accuracy on training set: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpWOwIP35geX"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**GENERAL HANDLERS**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVPdCOln4vBI"
      },
      "source": [
        "@bot.register_default_handler()\r\n",
        "def default_response(_):\r\n",
        "  return \"Sorry, I can't help you out with that ...\"\r\n",
        "\r\n",
        "@bot.register_handler('greetings')\r\n",
        "def say_hello(_):\r\n",
        "  return random.choice([\r\n",
        "  \"Hi!\",\r\n",
        "  \"Hi there!\",\r\n",
        "  \"Hello yourself!\"\r\n",
        "  ])\r\n",
        "\r\n",
        "@bot.register_handler('bye')\r\n",
        "def say_bye(_):\r\n",
        "  return random.choice([\r\n",
        "  \"Goodbye to you!\",\r\n",
        "  \"Nice meeting you, bye\",\r\n",
        "  \"Have a good day!\",\r\n",
        "  \"Goodbye\"\r\n",
        "  ])\r\n",
        "\r\n",
        "@bot.register_handler('thanks')\r\n",
        "def say_thanks(_):\r\n",
        "  return random.choice([\r\n",
        "  \"No, thank you!\",\r\n",
        "  \"It's what I do ...\",\r\n",
        "  \"At your service ...\",\r\n",
        "  \"Come back anytime\"\r\n",
        "  ])\r\n",
        "\r\n",
        "@bot.register_handler('chit-chat')\r\n",
        "def make_conversation(_):\r\n",
        "  return random.choice([\r\n",
        "  \"I'm fine, thanks for asking ...\",\r\n",
        "  \"The weather is fine, business is fine, I'm pretty well myself ...\",\r\n",
        "  \"I'm bored\",\r\n",
        "  \"All good, thanks\",\r\n",
        "  ])\r\n",
        "\r\n",
        "@bot.register_handler('rude')\r\n",
        "def be_rude(_):\r\n",
        "  return random.choice([\r\n",
        "  \"That's pretty rude of you. Come back with some manners ...\",\r\n",
        "  \"You're rude, leave me alone.\",\r\n",
        "  \"That's not very nice ...\",\r\n",
        "  \"You're not a nice person, are you?\",\r\n",
        "  ])\r\n",
        "\r\n",
        "@bot.register_handler('ask-info')\r\n",
        "def provide_info(_):\r\n",
        "  return random.choice([\r\n",
        "  \"I know a looot of songs, try me!\",\r\n",
        "  \"I can help you with songs and stuff ...\",\r\n",
        "  \"I am your father!\",\r\n",
        "  \"I come from the future!\",\r\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HYEyyRs5XzB"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**INFORMATIVE HANDLERS**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-6-kRw5WgT"
      },
      "source": [
        "@bot.register_handler('get-latest')\r\n",
        "def get_latest(_):\r\n",
        "  q = 'what are 2020 songs?'\r\n",
        "  res = client.query(q)\r\n",
        "  print(res)\r\n",
        "  res = next(res.results).text\r\n",
        "  \r\n",
        "  res = ','.join(set(list(map(lambda a: a.split('|').strip(), res.split('\\n')))[1]))\r\n",
        "  return res\r\n",
        "\r\n",
        "@bot.register_handler('what-singer')\r\n",
        "def what_singer(entities):\r\n",
        "  # change handler name\r\n",
        "  print(\"> Entities: {}\".format(entities))\r\n",
        "  try:\r\n",
        "    q = 'who is the singer of {} song?'.format(entities['song'])\r\n",
        "  except KeyError:\r\n",
        "    return 'Can not understand you.'\r\n",
        "  res = client.query(q)\r\n",
        "  res = next(res.results).text\r\n",
        "  res = ','.join(set(list(map(lambda a: a.split('|')[1].strip(), res.split('\\n')))[1:2]))\r\n",
        "  return res\r\n",
        "\r\n",
        "@bot.register_handler('what-release-year')\r\n",
        "def what_release_year(entities):\r\n",
        "  try:\r\n",
        "    q = 'who is the singer of {} song?'.format(entities['song'])\r\n",
        "  except KeyError:\r\n",
        "    return 'Can not understand you.'\r\n",
        "\r\n",
        "  res = client.query(q)\r\n",
        "  res = next(res.results).text\r\n",
        "  res = list(map(lambda a: a.split('|')[2].strip(), res.split('\\n')))[1].split(',')[-1]\r\n",
        "\r\n",
        "  return res\r\n",
        "\r\n",
        "@bot.register_handler('get-similar-songs')\r\n",
        "def get_similar_songs(entities):\r\n",
        "  # rename handler name\r\n",
        "  #\r\n",
        "  # get name of an actor\r\n",
        "  try:\r\n",
        "    q1 = 'who is the singer of {} song?'.format(entities['song'])\r\n",
        "  except KeyError:\r\n",
        "    return 'Can not understand you.'\r\n",
        "  # get from query\r\n",
        "  res = client.query(q1)\r\n",
        "  res = next(res.results).text\r\n",
        "  actor = list(map(lambda a: a.split('|')[1].strip(), res.split('\\n')))[1]\r\n",
        "  # pass request to wolfram and get list of songs\r\n",
        "  q2 = 'what are {} songs?'.format(actor)\r\n",
        "  res = client.query(q2)\r\n",
        "  res = next(res.results).text\r\n",
        "  res = ','.join(list(filter(lambda a: a != entities['song'], map(lambda a: a.split('|')[0].strip(), res.split('\\n')[1:])))[:-1])\r\n",
        "  return res\r\n",
        "\r\n",
        "@bot.register_handler('singer-songs')\r\n",
        "def get_actor_songs(entities):\r\n",
        "  # change handler name\r\n",
        "  try:\r\n",
        "    q = 'what are {} songs?'.format(entities['actor'])\r\n",
        "  except KeyError:\r\n",
        "    return 'Can not understand you.'\r\n",
        "  # -- code --\r\n",
        "  res = client.query(q)\r\n",
        "  res = next(res.results).text\r\n",
        "  res = ','.join(list(filter(lambda a: a != entities['song'], map(lambda a: a.split('|')[0].strip(), res.split('\\n')))))\r\n",
        "  return res\r\n",
        "\r\n",
        "@bot.register_handler('song-team')\r\n",
        "def song_cast(entities):\r\n",
        "  # ( !! ) similar to prev one \r\n",
        "  # change handler name to 'get_actors'\r\n",
        "  try:\r\n",
        "    q = 'who is the singer of {} song?'.format(entities['song'])\r\n",
        "  except KeyError:\r\n",
        "    return 'Can not understand you.'\r\n",
        "  # -- code -- \r\n",
        "  res = client.query(q)\r\n",
        "  res = next(res.results).text\r\n",
        "  res = ','.join(set(list(map(lambda a: a.split('|')[1].strip(), res.split('\\n')))[1:]))\r\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DCbNTJk8Ogq"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**WOLFRAM API SETUP**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FhD_NtY8UCo"
      },
      "source": [
        "client = wolframalpha.Client('TLRG9X-TTY79PQ7K8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZ6R0YsArW6"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "**DEMO**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIJSeDagAmXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "48bb5cc7-ed41-4c43-aebf-6d10c06c1bd0"
      },
      "source": [
        "while True:\r\n",
        "   line = input(\"You: \")\r\n",
        "   print(\"SongBot:\", bot.tell(line).strip(), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: Can I get some similar song to Halo?\n",
            "> Intent: get-similar-songs\n",
            "SongBot: Say My Name,Formation,Telephone,Irreplaceable,Bootylicious,Drunk in Love,XO,Listen,Survivor \n",
            "\n",
            "You: bye\n",
            "> Intent: bye\n",
            "SongBot: Goodbye to you! \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-24876993aba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SongBot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}